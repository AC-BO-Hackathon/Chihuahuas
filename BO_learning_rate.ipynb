{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AC-BO-Hackathon/Chihuahuas/blob/main/BO_batch_size.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "yVVBagOu1cMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Optimization for `batch_size`"
      ],
      "metadata": {
        "id": "it_UCDkSy4XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spektral -q\n",
        "!pip install scikit-optimize -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zzfCfAeuw1s",
        "outputId": "a25feb6d-d258-4b87-8847-4a55a3e17113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Enter the Size of the QM9 subset\n",
        "\n",
        "amount = 1000 # @param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "zmhg4nL8yHAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Optimización de hiperparámetros para un modelo GNN que predice una propiedad específica\n",
        "del conjunto de datos QM9, utilizando Scikit-Optimize (skopt).\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from spektral.data import BatchLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool, GraphMasking\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Integer\n",
        "from skopt.utils import use_named_args\n",
        "\n",
        "################################################################################\n",
        "# Configuración inicial\n",
        "################################################################################\n",
        "epochs = 10  # Número de épocas de entrenamiento\n",
        "batch_size= 20\n",
        "\n",
        "################################################################################\n",
        "# Carga y preparación de datos\n",
        "################################################################################\n",
        "dataset = QM9(amount)  # Usar amount=None para todo el conjunto de datos\n",
        "\n",
        "# Preparación de los datos para predecir solo la propiedad de índice 7\n",
        "for graph in dataset:\n",
        "    graph.y = graph.y[7].reshape(-1, 1)\n",
        "\n",
        "# División entrenamiento/prueba\n",
        "idxs = np.random.permutation(len(dataset))\n",
        "split = int(0.9 * len(dataset))\n",
        "idx_tr, idx_te = np.split(idxs, [split])\n",
        "dataset_tr, dataset_te = dataset[idx_tr], dataset[idx_te]\n",
        "\n",
        "################################################################################\n",
        "# Definición del modelo\n",
        "################################################################################\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.masking = GraphMasking()\n",
        "        self.conv1 = ECCConv(32, activation=\"relu\")\n",
        "        self.conv2 = ECCConv(32, activation=\"relu\")\n",
        "        self.global_pool = GlobalSumPool()\n",
        "        self.dense = Dense(1)  # Salida para una propiedad\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, e = inputs\n",
        "        x = self.masking(x)\n",
        "        x = self.conv1([x, a, e])\n",
        "        x = self.conv2([x, a, e])\n",
        "        output = self.global_pool(x)\n",
        "        output = self.dense(output)\n",
        "        return output\n",
        "\n",
        "################################################################################\n",
        "# Espacio de búsqueda para la optimización de hiperparámetros\n",
        "################################################################################\n",
        "space = [\n",
        "    Real(1e-3, 1e-1, name='learning_rate'),\n",
        "    #Integer(16, 128, name='batch_size')\n",
        "]\n",
        "\n",
        "################################################################################\n",
        "# Función objetivo para skopt\n",
        "################################################################################\n",
        "@use_named_args(space)\n",
        "def objective(learning_rate):\n",
        "    model = Net()\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\"mse\")\n",
        "\n",
        "    loader_tr = BatchLoader(dataset_tr, batch_size=batch_size, mask=True)\n",
        "    history = model.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs)\n",
        "\n",
        "    loader_te = BatchLoader(dataset_te, batch_size=batch_size, mask=True)\n",
        "    loss = model.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch)\n",
        "    return loss\n",
        "\n",
        "################################################################################\n",
        "# Ejecución de la optimización Bayesiana\n",
        "################################################################################\n",
        "result = gp_minimize(objective, space, n_calls=20, random_state=0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "675KmQdou5FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Resultados\n",
        "################################################################################\n",
        "print(f\"Mejor valor de pérdida: {result.fun}\")\n",
        "print(f\"Mejores hiperparámetros:\\n- Learning Rate: {result.x[0]} \")"
      ],
      "metadata": {
        "id": "Z3omO4vwxnxo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}